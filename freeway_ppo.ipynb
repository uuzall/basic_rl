{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "\tfrom IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RAM_mask = np.asarray([14, 16, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, run_name): \n",
    "\tdef thunk(): \n",
    "\t\tenv = gym.make(env_id)\n",
    "\t\tenv.action_space.seed(seed) \n",
    "\t\tenv.observation_space.seed(seed) \n",
    "\t\treturn env \n",
    "\treturn thunk() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0): \n",
    "\tnn.init.orthogonal_(layer.weight, std) \n",
    "\tnn.init.constant_(layer.bias, bias_const)\n",
    "\treturn layer \n",
    "\n",
    "class Agent(nn.Module): \n",
    "\tdef __init__(self, envs): \n",
    "\t\tsuper().__init__() \n",
    "\t\tself.critic = nn.Sequential(\n",
    "\t\t\tlayer_init(nn.Linear(RAM_mask.size, 64)), #np.array(envs.single_observation_space.shape).prod(), 64)), \n",
    "\t\t\tnn.Tanh(), \n",
    "\t\t\tlayer_init(nn.Linear(64, 64)), \n",
    "\t\t\tnn.Tanh(), \n",
    "\t\t\tlayer_init(nn.Linear(64, 1), std=1.0)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.actor = nn.Sequential(\n",
    "\t\t\tlayer_init(nn.Linear(RAM_mask.size, 64)), \n",
    "\t\t\tnn.Tanh(), \n",
    "\t\t\tlayer_init(nn.Linear(64, 64)), \n",
    "\t\t\tnn.Tanh(), \n",
    "\t\t\tlayer_init(nn.Linear(64, envs.single_action_space.n), std=0.01)\n",
    "\t\t)\n",
    "\n",
    "\tdef get_value(self, x): \n",
    "\t\treturn self.critic(x) \n",
    "\t\n",
    "\tdef get_action_and_value(self, x, action=None): \n",
    "\t\tlogits = self.actor(x) \n",
    "\t\tprobs = torch.distributions.categorical.Categorical(logits=logits) \n",
    "\t\tif action is None: \n",
    "\t\t\taction = probs.sample() \n",
    "\t\treturn action, probs.log_prob(action), probs.entropy(), self.critic(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337 \n",
    "num_envs = 4\n",
    "num_steps = 2048\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([\n",
    "    lambda: make_env('ALE/Freeway-ram-v5', seed+i, i, 'the first') for i in range(num_envs)\n",
    "])\n",
    "\n",
    "agent = Agent(envs).to(device) \n",
    "optimizer = optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [52:39<00:00, 12.34s/it, highest_score=23]\n"
     ]
    }
   ],
   "source": [
    "obs = torch.zeros((num_steps, num_envs, RAM_mask.size)).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(device) \n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device) \n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device) \n",
    "dones = torch.zeros((num_steps, num_envs)).to(device) \n",
    "values = torch.zeros((num_steps, num_envs)).to(device) \n",
    "\n",
    "total_timesteps = 64*4096\n",
    "bs = 1024\n",
    "gamma = 0.99\n",
    "update_epochs = 4 \n",
    "mini_bs = 64 #4 \n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.01 \n",
    "vf_coef = 0.5\n",
    "\n",
    "global_step = 0 \n",
    "next_obs = torch.tensor(envs.reset()[0][:, RAM_mask]/255., dtype=torch.float32).to(device) \n",
    "next_done = torch.zeros(num_envs).to(device) \n",
    "num_updates = total_timesteps // bs\n",
    "\n",
    "highest_score = 0 \n",
    "\n",
    "for update in (loop := trange(1, num_updates + 1)): \n",
    "\tscore = np.zeros(4)\n",
    "\tfor step in range(0, num_steps):\n",
    "\t\t# Simulate first, learn later. \n",
    "\t\tglobal_step = 1 * num_envs \n",
    "\t\tobs[step] = next_obs \n",
    "\t\tdones[step] = next_done \n",
    "\n",
    "\t\twith torch.no_grad(): \n",
    "\t\t\taction, logprob, _, value = agent.get_action_and_value(next_obs) \n",
    "\t\t\tvalues[step] = value.flatten()\n",
    "\t\tactions[step] = action \n",
    "\t\tlogprobs[step] = logprob \n",
    "\n",
    "\t\tnext_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "\t\tdone = np.bitwise_or(terminated, truncated).astype(np.float32)\n",
    "\t\tnext_obs, next_done = torch.tensor(next_obs[:, RAM_mask]/255., dtype=torch.float32).to(device), torch.tensor(done).to(device) \n",
    "\t\t\n",
    "\t\tscore[reward == np.ones(4)] += 1\n",
    "\n",
    "\t\t# for success\n",
    "\t\treward[reward == np.ones(4)] += 10 \n",
    "\t\t# for staying in one spot\n",
    "\t\treward[(obs[step][:, 0] - next_obs[:, 0]).detach().cpu().numpy() == np.zeros(4)] -= 1\n",
    "\t\t# for collision \n",
    "\t\treward[next_obs[:, 1] != np.ones(4)] -= 100\n",
    "\n",
    "\t\trewards[step] = torch.tensor(reward).to(device).view(-1) \n",
    "\tif score.max() > highest_score: \n",
    "\t\thighest_score = score.max()\n",
    "\t\n",
    "\twith torch.no_grad(): \n",
    "\t\tnext_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "\t\treturns = torch.zeros_like(rewards).to(device)\n",
    "\t\tfor t in reversed(range(num_steps)):\n",
    "\t\t\tif t == num_steps - 1: \n",
    "\t\t\t\t# if it is the last step then look at the next step to come (not stored)\n",
    "\t\t\t\tnext_non_terminal = 1.0 - next_done \n",
    "\t\t\t\tnext_return = next_value \n",
    "\t\t\telse: # if it is not the last step, it is already stored. \n",
    "\t\t\t\tnext_non_terminal = 1.0 - dones[t + 1]\n",
    "\t\t\t\tnext_return = returns[t + 1]\n",
    "\t\t\t# if it is done, 'next_non_terminal' is 0. \n",
    "\t\t\t# return = rewards_t + \\sum_{t+1}^{t+n} \\gamma(return_{t+1})(next_non_terminal)\n",
    "\t\t\treturns[t] = rewards[t] + gamma * next_non_terminal * next_return \n",
    "\t\tadvantages = returns - values \n",
    "\t\t\n",
    "\tb_obs = obs.reshape((-1, RAM_mask.size)) \n",
    "\tb_logprobs = logprobs.reshape(-1) \n",
    "\tb_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "\tb_advantages = advantages.reshape(-1) \n",
    "\tb_returns = returns.reshape(-1) \n",
    "\tb_values = values.reshape(-1) \n",
    "\tb_inds = np.arange(bs) \n",
    "\tclipfracs = list() \n",
    "\n",
    "\tfor epoch in range(update_epochs): \n",
    "\t\tnp.random.shuffle(b_inds)\n",
    "\t\tfor start in range(0, bs, mini_bs): \n",
    "\t\t\tend = start + mini_bs \n",
    "\t\t\tmb_inds = b_inds[start:end]\n",
    "\t\t\t_, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "\t\t\tlogratio = newlogprob - b_logprobs[mb_inds]\n",
    "\t\t\tratio = logratio.exp() # Ratio between current policy and old policy \n",
    "\n",
    "\t\t\twith torch.no_grad(): \n",
    "\t\t\t\told_approx_kl = (-logprob).mean() \n",
    "\t\t\t\tapprox_kl = ((ratio - 1) - logratio).mean() \n",
    "\t\t\t\tclipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "\t\t\tmb_advantages = b_advantages[mb_inds]\n",
    "\t\t\tmb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "\t\t\tpg_loss1 = -mb_advantages * ratio\n",
    "\t\t\tpg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "\t\t\tpg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\t\t\t# max because we are taking gradient ascent \n",
    "\n",
    "\t\t\tv_loss_unclipped = (newvalue - b_returns[mb_inds])**2 \n",
    "\t\t\tv_clipped = b_values[mb_inds] + torch.clamp(newvalue - b_values[mb_inds], -clip_coef, clip_coef)\n",
    "\t\t\tv_loss_clipped = (v_clipped - b_returns[mb_inds])**2 \n",
    "\t\t\tv_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "\t\t\tv_loss = 0.5 * v_loss_max.mean() \n",
    "\n",
    "\t\t\tentropy_loss = entropy.mean() \n",
    "\t\t\tloss = pg_loss - ent_coef * entropy_loss + vf_coef * v_loss\n",
    "\t\t\t# 'entropy_loss' maintains exploratory behavior of the model. \n",
    "\t\t\t# 'ent_coef' controls the explotory behavior \n",
    "\n",
    "\t\t\toptimizer.zero_grad() \n",
    "\t\t\tloss.backward() \n",
    "\t\t\tnn.utils.clip_grad_norm_(agent.parameters(), 0.5) \n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\tloop.set_postfix(highest_score=highest_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "one_env = gym.make('ALE/Freeway-ram-v5')\n",
    "state, info = one_env.reset() \n",
    "state = torch.tensor(state[RAM_mask], dtype=torch.float32, device=device).unsqueeze(0) \n",
    "\n",
    "total_score = 0 \n",
    "while True: \n",
    "  with torch.no_grad(): \n",
    "    action = agent.actor(state).argmax().item()\n",
    "  observation, rewards, terminated, truncated, _ = one_env.step(action)\n",
    "  state = torch.tensor(observation[RAM_mask], dtype=torch.float32, device=device)\n",
    "  if rewards == 1: \n",
    "    total_score += 1\n",
    "\n",
    "  if terminated or truncated: \n",
    "    break \n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), 'models/freeway_ppo/minus_100.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
