{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "\tfrom IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": \n",
    "\tprint(torch.cuda.get_device_name()) \n",
    "else: \n",
    "\tprint(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, run_name): \n",
    "\tdef thunk(): \n",
    "\t\tenv = gym.make(env_id)\n",
    "\t\t# if idx == 0: \n",
    "\t\t# env = gym.wrappers.RecordVideo(env, 'videos/pusher_ppo')\n",
    "\t\t# env = gym.wrappers.ClipAction(env) \n",
    "\t\t# env = gym.wrappers.NormalizeObservation(env) \n",
    "\t\t# env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "\t\t# env = gym.wrappers.NormalizeReward(env, gamma=0.99)\n",
    "\t\t# env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "\t\treturn env \n",
    "\treturn thunk() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diag_gaussian_distribution(): \n",
    "\tdef __init__(self, action_dim): \n",
    "\t\tsuper().__init__() \n",
    "\t\tself.action_dim = action_dim \n",
    "\t\tself.mean_actions = None \n",
    "\t\tself.log_std = None \n",
    "\t\t\n",
    "\tdef proba_distribution_net(self, latent_dim, log_std_init=0.0): \n",
    "\t\tmean_actions = nn.Linear(latent_dim, self.action_dim) \n",
    "\t\tlog_std = nn.Parameter(torch.ones(self.action_dim)*log_std_init, requires_grad=True) \n",
    "\t\treturn mean_actions, log_std \n",
    "\t\n",
    "\tdef proba_distribution(self, mean_actions, log_std): \n",
    "\t\taction_std = torch.ones_like(mean_actions) * log_std.exp() \n",
    "\t\tself.distribution = torch.distributions.normal.Normal(mean_actions, action_std)\n",
    "\t\treturn self \n",
    "\t\n",
    "\tdef log_prob(self, actions): \n",
    "\t\tlog_prob = self.distribution.log_prob(actions) \n",
    "\t\tif len(log_prob) > 1: \n",
    "\t\t\treturn log_prob.sum(dim=1) \n",
    "\t\telse: \n",
    "\t\t\treturn log_prob.sum() \n",
    "\t\t\n",
    "\tdef entropy(self): \n",
    "\t\tentropy = self.distribution.entropy() \n",
    "\t\tif len(entropy) > 1: \n",
    "\t\t\treturn entropy.sum(dim=1) \n",
    "\t\telse: \n",
    "\t\t\treturn entropy.sum() \n",
    "\t\t\n",
    "\tdef sample(self): \n",
    "\t\treturn self.distribution.rsample() \n",
    "\t\n",
    "\tdef mode(self): \n",
    "\t\treturn self.distribution.mean \n",
    "\t\n",
    "\tdef actions_from_params(self, mean_actions, log_std, deterministic=False): \n",
    "\t\tself.proba_distribution(mean_actions, log_std) \n",
    "\t\tif deterministic: \n",
    "\t\t\treturn self.mode() \n",
    "\t\treturn self.sample() \n",
    "\t\n",
    "\tdef log_prob_from_params(self, mean_actions, log_std): \n",
    "\t\tactions = self.actions_from_params(mean_actions, log_std) \n",
    "\t\tlog_prob = self.log_prob(actions) \n",
    "\t\treturn actions, log_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0): \n",
    "\tnn.init.orthogonal_(layer.weight, std) \n",
    "\tnn.init.constant_(layer.bias, bias_const)\n",
    "\treturn layer \n",
    "\n",
    "class Agent(nn.Module): \n",
    "\tdef __init__(self, envs): \n",
    "\t\tsuper().__init__() \n",
    "\t\tself.state_dist = diag_gaussian_distribution(action_dim=np.array(envs.single_action_space.shape).prod())\n",
    "\n",
    "\t\tself.critic = nn.Sequential(\n",
    "\t\t\tlayer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), hidden_dim)), \n",
    "\t\t\tnn.Tanh(), \n",
    "\t\t\tlayer_init(nn.Linear(hidden_dim, hidden_dim)), \n",
    "\t\t\tnn.Tanh(), \n",
    "\t\t\tlayer_init(nn.Linear(hidden_dim, 1), std=1.0)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.actor = nn.Sequential(\n",
    "\t\t\tlayer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), hidden_dim)), \n",
    "\t\t\tnn.Tanh(), \n",
    "\t\t\tlayer_init(nn.Linear(hidden_dim, hidden_dim)), \n",
    "\t\t\tnn.Tanh(), \n",
    "\t\t)\n",
    "\n",
    "\t\tself.actor_mean, self.actor_logstd = self.state_dist.proba_distribution_net(latent_dim=hidden_dim)\n",
    "\t\tself.actor_mean = layer_init(self.actor_mean, std=0.01)\n",
    "\n",
    "\tdef get_value(self, x): \n",
    "\t\treturn self.critic(x) \n",
    "\t\n",
    "\tdef get_action_and_value(self, x, pre_action=None): \n",
    "\t\taction = self.actor(x) \n",
    "\t\taction_mean = self.actor_mean(action) \n",
    "\t\tactions, log_prob = self.state_dist.log_prob_from_params(mean_actions=action_mean, log_std=self.actor_logstd)\n",
    "\t\tentropy = self.state_dist.entropy() \n",
    "\t\tif pre_action is None: \n",
    "\t\t\treturn actions, log_prob, entropy, self.critic(x) \n",
    "\t\telse: \n",
    "\t\t\tlog_prob = self.state_dist.log_prob(pre_action)\n",
    "\t\t\treturn pre_action, log_prob, entropy, self.critic(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337 \n",
    "num_envs = 1\n",
    "num_steps = 2048\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([\n",
    "    lambda: make_env('Pusher-v4', seed+i, i, 'the first') for i in range(num_envs)\n",
    "])\n",
    "\n",
    "agent = Agent(envs).to(device) \n",
    "optimizer = optim.Adam(agent.parameters(), lr=2.5e-4, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch_size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1953/1953 [4:08:22<00:00,  7.63s/it, highest_score=-.0848, loss=2.58, policy_gradient_loss=-.0355, value_loss=5.23, zscore=[-0.1018188363237815]]         \n"
     ]
    }
   ],
   "source": [
    "obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(device) \n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device) \n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device) \n",
    "dones = torch.zeros((num_steps, num_envs)).to(device) \n",
    "values = torch.zeros((num_steps, num_envs)).to(device) \n",
    "\n",
    "gae_lambda = 1.0\n",
    "total_timesteps = 4000000 #(768 * 2)\n",
    "gamma = 0.99\n",
    "update_epochs = 10\n",
    "mini_bs = 32 # number of mini batches \n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.0 # default: 0.01\n",
    "vf_coef = 0.5\n",
    "bs = int(num_envs * num_steps)\n",
    "minibatch_size = int(bs / mini_bs) # 64\n",
    "print(f'minibatch_size: {minibatch_size}')\n",
    "\n",
    "global_step = 0 \n",
    "next_obs = torch.tensor(envs.reset()[0], dtype=torch.float32).to(device) \n",
    "next_done = torch.zeros(num_envs).to(device) \n",
    "num_updates = total_timesteps // bs\n",
    "\n",
    "highest_score = -999\n",
    "\n",
    "for update in (loop := trange(1, num_updates + 1)): \n",
    "\tscore = np.zeros(num_envs)\n",
    "\tfor step in range(0, num_steps):\n",
    "\t\t# Simulate first, learn later. \n",
    "\t\tglobal_step += 1 * num_envs \n",
    "\t\tobs[step] = next_obs \n",
    "\t\tdones[step] = next_done \n",
    "\n",
    "\t\twith torch.no_grad(): \n",
    "\t\t\taction, logprob, _, value = agent.get_action_and_value(next_obs) \n",
    "\t\t\tvalues[step] = value.flatten()\n",
    "\t\tactions[step] = action \n",
    "\t\tlogprobs[step] = logprob \n",
    "\n",
    "\t\tnext_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "\t\tdone = np.bitwise_or(terminated, truncated).astype(np.float32)\n",
    "\t\tnext_obs, next_done = torch.tensor(next_obs, dtype=torch.float32).to(device), torch.tensor(done).to(device) \n",
    "\t\t\n",
    "\t\tscore = reward.tolist()\n",
    "\n",
    "\t\trewards[step] = torch.tensor(reward).to(device).view(-1) \n",
    "\t\t\n",
    "\t\tif max(score) > highest_score: \n",
    "\t\t\thighest_score = max(score)\n",
    "\t\t\ttorch.save(agent.state_dict(), f'models/pusher_ppo_diag/best_performing.pth')\n",
    "\t\n",
    "\twith torch.no_grad(): \n",
    "\t\tnext_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "\t\tadvantages = torch.zeros_like(rewards).to(device)\n",
    "\t\tlast_gae_lam = 0 \n",
    "\t\tfor t in reversed(range(num_steps)):\n",
    "\t\t\tif t == num_steps - 1: \n",
    "\t\t\t\t# if it is the last step then look at the next step to come (not stored)\n",
    "\t\t\t\tnext_non_terminal = 1.0 - next_done \n",
    "\t\t\t\tnext_values = next_value \n",
    "\t\t\telse: # if it is not the last step, it is already stored. \n",
    "\t\t\t\tnext_non_terminal = 1.0 - dones[t + 1]\n",
    "\t\t\t\tnext_values = values[t + 1]\n",
    "\t\t\t# if it is done, 'next_non_terminal' is 0. \n",
    "\t\t\t# return = rewards_t + \\sum_{t+1}^{t+n} \\gamma(return_{t+1})(next_non_terminal)\n",
    "\t\t\tdelta = rewards[t] + gamma * next_values * next_non_terminal - values[t]\n",
    "\t\t\tadvantages[t] = last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "\t\treturns = advantages + values \n",
    "\t\t\n",
    "\tb_obs = obs.reshape((-1,) + envs.single_observation_space.shape) \n",
    "\tb_logprobs = logprobs.reshape(-1) \n",
    "\tb_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "\tb_advantages = advantages.reshape(-1) \n",
    "\tb_returns = returns.reshape(-1) \n",
    "\tb_values = values.reshape(-1) \n",
    "\tb_inds = np.arange(bs) \n",
    "\tclipfracs = list() \n",
    "\n",
    "\tfor epoch in range(update_epochs): \n",
    "\t\tnp.random.shuffle(b_inds)\n",
    "\t\tfor start in range(0, bs, minibatch_size): \n",
    "\t\t\tend = start + minibatch_size\n",
    "\t\t\tmb_inds = b_inds[start:end]\n",
    "\t\t\t_, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "\t\t\tlogratio = newlogprob - b_logprobs[mb_inds]\n",
    "\t\t\tratio = logratio.exp() # Ratio between current policy and old policy \n",
    "\n",
    "\t\t\twith torch.no_grad(): \n",
    "\t\t\t\t# old_approx_kl = (-logprob).mean() \n",
    "\t\t\t\t# approx_kl = ((ratio - 1) - logratio).mean() \n",
    "\t\t\t\tclipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "\t\t\tmb_advantages = b_advantages[mb_inds]\n",
    "\t\t\t# normalize advantage \n",
    "\t\t\tmb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "\t\t\tpg_loss1 = mb_advantages * ratio \n",
    "\t\t\tpg_loss2 = mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "\t\t\tpg_loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "\t\t\t# max because we are taking gradient ascent \n",
    "\n",
    "\t\t\tnewvalue = newvalue.view(-1)\n",
    "\t\t\t# newvalue = b_values[mb_inds] + torch.clamp(newvalue - b_values[mb_inds], -clip_coef, clip_coef)\n",
    "\t\t\tv_loss = F.mse_loss(b_returns[mb_inds], newvalue)\n",
    "\n",
    "\t\t\tentropy_loss = entropy.mean() \n",
    "\t\t\tloss = pg_loss - ent_coef * entropy_loss + vf_coef * v_loss\n",
    "\t\t\t# 'entropy_loss' maintains exploratory behavior of the model. \n",
    "\t\t\t# 'ent_coef' controls the exploratory behavior \n",
    "\n",
    "\t\t\toptimizer.zero_grad() \n",
    "\t\t\tloss.backward() \n",
    "\t\t\tnn.utils.clip_grad_norm_(agent.parameters(), 0.5) \n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\tloop.set_postfix(highest_score=highest_score, zscore=score, value_loss=v_loss.item(), policy_gradient_loss=pg_loss.item(), loss=loss.item()) \n",
    "\tif update % 64 == 0: \n",
    "\t\ttorch.save(agent.state_dict(), f'models/pusher_ppo_diag/{update}.pth')\n",
    "\n",
    "torch.save(agent.state_dict(), f'models/pusher_ppo_diag/final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts with 1.22 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
