{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "\tfrom IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Freeway-ram-v5')\n",
    "\n",
    "RAM_mask = np.asarray([14, 16, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class replay_memory(object): \n",
    "\tdef __init__(self, capacity): \n",
    "\t\tself.memory = deque(list(), maxlen=capacity) \n",
    "\t\t\n",
    "\tdef push(self, *args): \n",
    "\t\tself.memory.append(transition(*args))\n",
    "\t\n",
    "\tdef sample(self, batch_size): \n",
    "\t\treturn random.sample(self.memory, batch_size) \n",
    "\t\n",
    "\tdef __len__(self): \n",
    "\t\treturn len(self.memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state): \n",
    "\tglobal steps_done\n",
    "\tsample = random.random() \n",
    "\teps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1. * steps_done / eps_decay) \n",
    "\tsteps_done += 1 \n",
    "\tif sample > eps_threshold: \n",
    "\t\twith torch.no_grad(): \n",
    "\t\t\treturn policy_net(state).argmax(keepdims=True)\n",
    "\telse: \n",
    "\t\treturn torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def plot_durations(show_result=False): \n",
    "\t_, ax1 = plt.subplots() \n",
    "\tdurations_t = torch.tensor(episode_duration, dtype=torch.float)\n",
    "\treward_t = torch.tensor(rewards, dtype=torch.float)\n",
    "\tif show_result: \n",
    "\t\tplt.title('Result')\n",
    "\telse: \n",
    "\t\t# plt.clf()\n",
    "\t\tplt.title('Training...')\n",
    "\tax1.set_xlabel('Episode')\n",
    "\tax1.set_ylabel('Duration')\n",
    "\tax1.plot(durations_t.numpy(), color='blue')\n",
    "\n",
    "\tax2 = ax1.twinx()\n",
    "\tax2.set_ylabel('Reward')\n",
    "\tax2.plot(reward_t.numpy(), color='red')\n",
    "\n",
    "\tif is_ipython: \n",
    "\t\tif not show_result: \n",
    "\t\t\tdisplay.display(plt.gcf()) \n",
    "\t\t\tdisplay.clear_output(wait=True) \n",
    "\t\telse: \n",
    "\t\t\tdisplay.display(plt.gcf())\n",
    "\n",
    "def optimize_model(): \n",
    "\tif len(memory) < bs: \n",
    "\t\treturn False\n",
    "\ttransitions = memory.sample(bs)\n",
    "\tbatch = transition(*zip(*transitions))\n",
    "\tnon_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "\tnon_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "\tstate_batch = torch.cat(batch.state)\n",
    "\taction_batch = torch.cat(batch.action)\n",
    "\treward_batch = torch.cat(batch.reward) \n",
    "\n",
    "\tstate_action_values = policy_net(state_batch).gather(1, action_batch) \n",
    "\t\n",
    "\tnext_state_values = torch.zeros(bs, device=device)\n",
    "\n",
    "\twith torch.no_grad(): \n",
    "\t\tnext_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\t\n",
    "\texpected_state_action_values = (next_state_values * gamma) + reward_batch \n",
    "\n",
    "\tloss = loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\toptimizer.zero_grad() \n",
    "\tloss.backward() \n",
    "\n",
    "\tnn.utils.clip_grad_value_(policy_net.parameters(), 100) \n",
    "\toptimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn(nn.Module): \n",
    "\tdef __init__(self, n_action=env.action_space.n): \n",
    "\t\tsuper().__init__() \n",
    "\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(12, 8), nn.ReLU(), \n",
    "\t\t\tnn.Linear(8, 4), nn.ReLU(), \n",
    "\t\t\tnn.Linear(4, n_action)\n",
    "\t\t)\n",
    "\t\n",
    "\tdef forward(self, x): \n",
    "\t\tout = self.net(x/255.) \n",
    "\t\treturn out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations 500/500: 100%|██████████| 500/500 [3:00:54<00:00, 21.71s/it, highest_score=27, score=21]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bs = 128 \n",
    "gamma = 0.99 \n",
    "eps_start, eps_end = 0.9, 0.05 \n",
    "eps_decay = 100000\n",
    "tau = 0.005 \n",
    "lr = 1e-4  \n",
    "\n",
    "state, info = env.reset() \n",
    "\n",
    "policy_net = dqn().to(device) \n",
    "target_net = dqn().to(device) \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "memory = replay_memory(100000) \n",
    "\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "num_episodes = 500\n",
    "episode_duration = list() \n",
    "rewards, rewards_agg = list(), 0 \n",
    "\n",
    "score = 0 \n",
    "highest_score = 0 \n",
    "\n",
    "for i_episode in (loop := trange(num_episodes)): \n",
    "\tstate, info = env.reset() \n",
    "\tstate = torch.tensor(state[RAM_mask], dtype=torch.float32, device=device).unsqueeze(0) \n",
    "\tfor t in count(): \n",
    "\t\taction = select_action(state)\n",
    "\t\tobservation, reward, terminated, truncated, _ = env.step(action.item()) \n",
    "\t\t\n",
    "\t\tif reward == 1: \n",
    "\t\t\treward += 499\n",
    "\t\t\tscore += 1\n",
    "\t\tif observation[14] - state[0, 0] == 0: \n",
    "\t\t\treward -= 1 \n",
    "\t\tif observation[16] != 255: \n",
    "\t\t\treward -= 100\n",
    "\n",
    "\t\treward = torch.tensor([reward], device=device)\n",
    "\n",
    "\t\tif terminated: \n",
    "\t\t\tnext_state = None \n",
    "\t\telse: \n",
    "\t\t\tnext_state = torch.tensor(observation[RAM_mask], dtype=torch.float32, device=device).unsqueeze(0) \n",
    "\n",
    "\t\tmemory.push(state, action, next_state, reward) \n",
    "\n",
    "\t\tstate = next_state\n",
    "\n",
    "\t\toptimize_model() \n",
    "\n",
    "\t\tif t % 50 == 0: \n",
    "\t\t\t# target_net_state_dict = target_net.state_dict() \n",
    "\t\t\t# policy_net_state_dict = policy_net.state_dict() \n",
    "\t\t\t# for key in policy_net_state_dict: \n",
    "\t\t\t# \ttarget_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "\n",
    "\t\t\t# target_net.load_state_dict(target_net_state_dict) \n",
    "\t\t\ttarget_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\t\tif terminated or truncated: \n",
    "\t\t\t# state, info = env.reset() \n",
    "\t\t\t# state = torch.tensor(state[RAM_mask], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\t\t\tif score > highest_score: \n",
    "\t\t\t\ttorch.save(policy_net.state_dict(), f'models/freeway/{i_episode}_{t}_highest_score.pth')\n",
    "\t\t\t\thighest_score = score\n",
    "\t\t\tscore = 0 \n",
    "\t\t\tbreak\n",
    "\t\trewards.append(reward.item())\n",
    "\t\tloop.set_description(f'Iterations {i_episode+1}/{num_episodes}')\n",
    "\t\tloop.set_postfix(score=score, highest_score=highest_score)\n",
    "\t\n",
    "\tif (i_episode + 1) % 10 == 0: \n",
    "\t\ttorch.save(policy_net.state_dict(), f'models/freeway/episode_{i_episode+1}.pth')\n",
    "\n",
    "print('Complete')\n",
    "# plot_durations(show_result=True) \n",
    "# plt.ioff() \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
