{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "\tfrom IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = 'cpu' #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class replay_memory(object): \n",
    "\tdef __init__(self, capacity): \n",
    "\t\tself.memory = deque([], maxlen=capacity)\n",
    "\t\n",
    "\tdef push(self, *args): \n",
    "\t\tself.memory.append(transition(*args))\n",
    "\t\n",
    "\tdef sample(self, batch_size): \n",
    "\t\treturn random.sample(self.memory, batch_size) \n",
    "\t\n",
    "\tdef __len__(self): \n",
    "\t\treturn len(self.memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn(nn.Module): \n",
    "\tdef __init__(self, n_observations, n_actions): \n",
    "\t\tsuper().__init__() \n",
    "\t\t\n",
    "\t\tself.layer1 = nn.Linear(n_observations, 128) \n",
    "\t\tself.layer2 = nn.Linear(128, 128) \n",
    "\t\tself.layer3 = nn.Linear(128, n_actions) \n",
    "\n",
    "\tdef forward(self, x): \n",
    "\t\tx = F.relu(self.layer1(x)) \n",
    "\t\tx = F.relu(self.layer2(x)) \n",
    "\t\treturn self.layer3(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128 \n",
    "gamma = 0.99 \n",
    "eps_start, eps_end = 0.9, 0.05 \n",
    "eps_decay = 1000 \n",
    "tau = 0.005 \n",
    "lr = 1e-4 \n",
    "\n",
    "n_actions = env.action_space.n \n",
    "state, info = env.reset() \n",
    "n_observations = len(state) \n",
    "\n",
    "policy_net = dqn(n_observations, n_actions).to(device) \n",
    "target_net = dqn(n_observations, n_actions).to(device) \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "memory = replay_memory(10000) \n",
    "\n",
    "steps_done = 0 \n",
    "\n",
    "def select_action(state): \n",
    "\tglobal steps_done \n",
    "\tsample = random.random() \n",
    "\teps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1. * steps_done / eps_decay) \n",
    "\tsteps_done += 1 \n",
    "\tif sample > eps_threshold: \n",
    "\t\twith torch.no_grad(): \n",
    "\t\t\treturn policy_net(state).argmax(keepdims=True)#.max(1)[1].view(-1, 1)\n",
    "\telse: \n",
    "\t\treturn torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def plot_durations(show_result=False): \n",
    "\t_, ax1 = plt.subplots() \n",
    "\tdurations_t = torch.tensor(episode_duration, dtype=torch.float)\n",
    "\treward_t = torch.tensor(rewards, dtype=torch.float)\n",
    "\tif show_result: \n",
    "\t\tplt.title('Result')\n",
    "\telse: \n",
    "\t\t# plt.clf()\n",
    "\t\tplt.title('Training...')\n",
    "\tax1.set_xlabel('Episode')\n",
    "\tax1.set_ylabel('Duration')\n",
    "\tax1.plot(durations_t.numpy(), color='blue')\n",
    "\n",
    "\tax2 = ax1.twinx()\n",
    "\tax2.set_ylabel('Reward')\n",
    "\tax2.plot(reward_t.numpy(), color='red')\n",
    "\n",
    "\t# if len(durations_t) >= 100: \n",
    "\t# \tmeans = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "\t# \tmeans = torch.cat((torch.zeros(99), means)) \n",
    "\t# \tax1.plot(means.numpy(), color='black')\n",
    "\t\n",
    "\t# plt.pause(0.001) \n",
    "\n",
    "\tif is_ipython: \n",
    "\t\tif not show_result: \n",
    "\t\t\tdisplay.display(plt.gcf()) \n",
    "\t\t\tdisplay.clear_output(wait=True) \n",
    "\t\telse: \n",
    "\t\t\tdisplay.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "def optimize_model(): \n",
    "\tif len(memory) < bs: \n",
    "\t\treturn False\n",
    "\ttransitions = memory.sample(bs) \n",
    "\tbatch = transition(*zip(*transitions))\n",
    "\tnon_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "\tnon_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\tstate_batch = torch.cat(batch.state) \n",
    "\taction_batch = torch.cat(batch.action)\n",
    "\treward_batch = torch.cat(batch.reward) \n",
    "\n",
    "\tstate_action_values = policy_net(state_batch).gather(1, action_batch) \n",
    "\t\n",
    "\tnext_state_values = torch.zeros(bs, device=device)\n",
    "\n",
    "\twith torch.no_grad(): \n",
    "\t\tnext_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\t\n",
    "\texpected_state_action_values = (next_state_values * gamma) + reward_batch \n",
    "\n",
    "\tloss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\toptimizer.zero_grad() \n",
    "\tloss.backward() \n",
    "\n",
    "\tnn.utils.clip_grad_value_(policy_net.parameters(), 100) \n",
    "\toptimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 600\n",
    "episode_duration = list() \n",
    "rewards, reward_agg = list(), 0\n",
    "\n",
    "for i_episode in range(num_episodes): \n",
    "\tstate, info = env.reset() \n",
    "\tstate = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) \n",
    "\t\n",
    "\tfor t in count():\n",
    "\t\taction = select_action(state) \n",
    "\t\tobservation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\t\treward_agg += reward\n",
    "\t\treward = torch.tensor([reward], device=device)\n",
    "\t\tdone = terminated or truncated \n",
    "\n",
    "\t\tif terminated: \n",
    "\t\t\tnext_state = None \n",
    "\t\telse: \n",
    "\t\t\tnext_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0) \n",
    "\n",
    "\t\tmemory.push(state, action, next_state, reward) \n",
    "\n",
    "\t\tstate = next_state \n",
    "\n",
    "\t\toptimize_model() \n",
    "\n",
    "\t\ttarget_net_state_dict = target_net.state_dict() \n",
    "\t\tpolicy_net_state_dict = policy_net.state_dict() \n",
    "\t\tfor key in policy_net_state_dict: \n",
    "\t\t\ttarget_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "\t\t\n",
    "\t\ttarget_net.load_state_dict(target_net_state_dict) \n",
    "\n",
    "\t\tif done: \n",
    "\t\t\tepisode_duration.append(t + 1) \n",
    "\t\t\tplot_durations() \n",
    "\t\t\tbreak \n",
    "\trewards.append(reward_agg/t)\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True) \n",
    "plt.ioff() \n",
    "plt.show()\n",
    "# duration: blue \n",
    "# reward: red "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'models/policy_net_lunar_lander')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
